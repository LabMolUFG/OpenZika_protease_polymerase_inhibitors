{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary QSAR in Python - LabMol\n",
    "\n",
    "Developed by: \n",
    "#### José Teófilo Moreira Filho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='white'> Model building with featmorgan fingerprint and RF, SVM and LightGBM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"date of creation:\\n\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages \n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, matthews_corrcoef, roc_curve, precision_recall_curve, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import auc as mauc\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from mordred import Calculator, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check initial number of compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading molecules and activity (0 and 1) from SDF\n",
    "fname = \"../data/1224857_bruno_imbalanced_prep.sdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = PandasTools.LoadSDF(fname, smilesName='SMILES', includeFingerprints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0[\"is_active\"] = np.where(df0[\"Outcome\"] == \"Active\", 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count = df0.is_active.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "target_count.plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the table as SD file\n",
    "#PandasTools.WriteSDF(df0, '../data/w2_isactive_5445.sdf', properties=df0.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprint and descriptors calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading molecules and activity (0 and 1) from SDF\n",
    "fname = \"../data/1224857_bruno_imbalanced_prep.sdf\"\n",
    "\n",
    "mols = []\n",
    "y = []\n",
    "for mol in Chem.SDMolSupplier(fname):\n",
    "    if mol is not None:\n",
    "        mols.append(mol)\n",
    "        y.append(mol.GetIntProp(\"is_active\")) # target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[:,-1:].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptors (fingerprints) and convert them into numpy array\n",
    "\n",
    "# generate binary FeatMorgan fingerprint with radius 2\n",
    "fp = [AllChem.GetMorganFingerprintAsBitVect(m, 2, 2048, useFeatures=True) for m in mols]\n",
    "\n",
    "def rdkit_numpy_convert(fp):\n",
    "    output = []\n",
    "    for f in fp:\n",
    "        arr = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(f, arr)\n",
    "        output.append(arr)\n",
    "    return np.asarray(output)\n",
    "# Convert fingerprint to numpy array\n",
    "x_fp = rdkit_numpy_convert(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fingerprint to Pandas DataFram,e\n",
    "x_fp = pd.DataFrame(x_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create descriptor calculator with all descriptors\n",
    "calc = Calculator(descriptors, ignore_3D=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of descriptors\n",
    "len(calc.descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mordred descriptor calculation\n",
    "df = calc.pandas(mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of compounds and descriptors\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data between 0 and 1\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df2 = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of compounds and descriptors\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows of the scaled DataFrame of descriptors\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows of the first DataFrame of descriptors\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperate the name of columns lost during scaling\n",
    "df2.columns=df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the existence of NaN values\n",
    "df2.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with NaN values\n",
    "df3 = df2.dropna(axis = 1, how ='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of compounds and descriptors\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the existence of NaN values\n",
    "df3.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows \n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove High correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant descriptors\n",
    "constant_filter = VarianceThreshold(threshold=0.01)  \n",
    "constant_filter.fit(df3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of not constant descriptors\n",
    "len(df3.columns[constant_filter.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of constant descriptors\n",
    "constant_columns = [column for column in df3.columns\n",
    "                    if column not in df3.columns[constant_filter.get_support()]]\n",
    "\n",
    "print(len(constant_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant columns\n",
    "df4 = df3.drop(labels=constant_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows \n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of compounds and descriptors\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlated_features = set()\n",
    "correlation_matrix = df4.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.90\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated columns\n",
    "df_clean = df4.drop(labels=correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows \n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join fingerprint to Mordred descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join fingerprint to Mordred descriptors\n",
    "x = df_clean.join(x_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fisrt five rows\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of compounds and descriptors\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x))\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folds for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Model building - Bayesian hyperparameter search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(geometric_mean_score)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt_rf = BayesSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {'max_features': ['auto', 'sqrt'],\n",
    "    'n_estimators': [100, 1000],\n",
    "    \"max_depth\": [2, 100],\n",
    "    'min_samples_leaf': [1,20], \n",
    "    'min_samples_split': [2, 20]\n",
    "    },\n",
    "    n_iter=50, # Number of parameter settings that are sampled\n",
    "    cv=cv,\n",
    "    scoring = scorer,\n",
    "    verbose=0,\n",
    "    refit= True, # Refit the best estimator with the entire dataset.\n",
    "    random_state=42, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "opt_rf.fit(x, y)\n",
    "\n",
    "print(\"Best parameters: %s\" % opt_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = []\n",
    "#indexes = []\n",
    "y_test_all = []\n",
    "\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    rf_clf = RandomForestClassifier(**opt_rf.best_params_) # model with best parameters\n",
    "    X_train_folds = x[train_index] # descritors train split\n",
    "    y_train_folds = np.array(y)[train_index.astype(int)] # label train split\n",
    "    X_test_fold = x[test_index] # descritors test split\n",
    "    y_test_fold = np.array(y)[test_index.astype(int)] # label test split\n",
    "    \n",
    "    \n",
    "    rf_clf.fit(X_train_folds, y_train_folds) # train fold\n",
    "    y_pred = rf_clf.predict_proba(X_test_fold) # test fold\n",
    "    probs_classes.append(y_pred) # all predictions for test folds\n",
    "    y_test_all.append(y_test_fold) # all folds' labels \n",
    "#   indexes.append(test_index) # all tests indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of each fold\n",
    "fold_1_pred = (probs_classes[0][:, 1] > 0.5).astype(int)\n",
    "fold_2_pred = (probs_classes[1][:, 1] > 0.5).astype(int)\n",
    "fold_3_pred = (probs_classes[2][:, 1] > 0.5).astype(int)\n",
    "fold_4_pred = (probs_classes[3][:, 1] > 0.5).astype(int)\n",
    "fold_5_pred = (probs_classes[4][:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experimental values of each fold\n",
    "fold_1_exp = y_test_all[0]\n",
    "fold_2_exp = y_test_all[1]\n",
    "fold_3_exp = y_test_all[2]\n",
    "fold_4_exp = y_test_all[3]\n",
    "fold_5_exp = y_test_all[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacc1 = metrics.balanced_accuracy_score(fold_1_exp, fold_1_pred) # balanced accuracy fold 1\n",
    "bacc2 = metrics.balanced_accuracy_score(fold_2_exp, fold_2_pred) # balanced accuracy fold 2\n",
    "bacc3 = metrics.balanced_accuracy_score(fold_3_exp, fold_3_pred) # balanced accuracy fold 3\n",
    "bacc4 = metrics.balanced_accuracy_score(fold_4_exp, fold_4_pred) # balanced accuracy fold 4\n",
    "bacc5 = metrics.balanced_accuracy_score(fold_5_exp, fold_5_pred) # balanced accuracy fold 5\n",
    "print(\"Balanced accuracy (fold 1) = \", bacc1)\n",
    "print(\"Balanced accuracy (fold 2) = \", bacc2)\n",
    "print(\"Balanced accuracy (fold 3) = \", bacc3)\n",
    "print(\"Balanced accuracy (fold 4) = \", bacc4)\n",
    "print(\"Balanced accuracy (fold 5) = \", bacc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mean performance of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = np.concatenate(probs_classes)    \n",
    "y_experimental = np.concatenate(y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncalibrated model predictions\n",
    "pred_rf = (probs_classes[:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics - featmorgan-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_statistics(y,pred):\n",
    "    # save confusion matrix and slice into four pieces\n",
    "    confusion = confusion_matrix(y, pred)\n",
    "    #[row, column]\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    \n",
    "    # Plot confusion\n",
    "    #plt.figure(figsize=(5,5))\n",
    "    #sns.heatmap(confusion, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    #plt.ylabel('Actual label');\n",
    "    #plt.xlabel('Predicted label');\n",
    "    #title = \"Confusion matrix\"\n",
    "    #plt.title(title, size = 15);\n",
    "    \n",
    "    # calc statistics\n",
    "    classification_error = 1 - accuracy_score(y, pred) #Classification error or misclassification rate\n",
    "    accuracy = accuracy_score(y, pred) #accuracy\n",
    "    mcc = matthews_corrcoef(y, pred) #mcc\n",
    "    kappa = cohen_kappa_score(y, pred) #kappa\n",
    "    sensitivity = recall_score(y, pred) #Sensitivity\n",
    "    specificity = TN / (TN + FP) #Specificity\n",
    "    false_positive_rate = FP / float(TN + FP) #False positive rate (alfa)\n",
    "    false_negative_rate = FN / float(TP+FN) #False negative rate (beta)\n",
    "    precision = TP / float(TP + FP) #Precision\n",
    "    positive_pred_value = TP / float(TP + FP) #PPV\n",
    "    negative_pred_value = TN / float(TN + FN) #NPV\n",
    "    auc = roc_auc_score(y, pred) #AUC\n",
    "    bacc = balanced_accuracy_score(y, pred) # balanced accuracy\n",
    "    f1 = f1_score(y, pred) # F1-score\n",
    "\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "    print(\"MCC = \", mcc)\n",
    "    print(\"Kappa = \", kappa)\n",
    "    print(\"Sensitivity = \", sensitivity)\n",
    "    print(\"Specificity = \", specificity)\n",
    "    print(\"Precision = \", precision)\n",
    "    print(\"PPV = \", positive_pred_value)\n",
    "    print(\"NPV = \", negative_pred_value)\n",
    "    print(\"False positive rate = \", false_positive_rate)\n",
    "    print(\"False negative rate = \", false_negative_rate)\n",
    "    print(\"AUC = \",roc_auc_score(y, pred))\n",
    "    print(\"Classification error = \", classification_error)\n",
    "    print(\"Balanced accuracy = \", bacc)\n",
    "    print(\"F1-score = \", f1)\n",
    "    \n",
    "    #converting calculated metrics into a pandas dataframe to compare all models at the final\n",
    "    statistics = pd.DataFrame({'Bal-acc': bacc, \"Sensitivity\": sensitivity, \"Specificity\": specificity,\"PPV\": positive_pred_value, \n",
    "           \"NPV\": negative_pred_value, 'Kappa': kappa, 'AUC': auc, 'MCC': mcc, 'Accuracy': accuracy, \n",
    "           \"Classification error\": classification_error,\"False positive rate\": false_positive_rate, \n",
    "           \"False negative rate\": false_negative_rate, \"Precision\": precision, 'F1-score': f1,}, index=[0])\n",
    "    return(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_featmorgan_r2_2048_rf\"\n",
    "\n",
    "result_type = \"uncalibrated\"\n",
    "\n",
    "metrics_rf_uncalibrated = statistics\n",
    "metrics_rf_uncalibrated['model'] = model_type\n",
    "metrics_rf_uncalibrated['result_type'] = result_type\n",
    "metrics_rf_uncalibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model calibatrion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "probs = probs_classes[:, 1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_experimental, probs, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_model_hybrid_mordred_featmorgan_r2_2048_rf_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ROC-Curve and Gmean to select a threshold for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_experimental, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='RF')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_curve_hybrid_mordred_featmorgan_r2_2048_rf_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_roc = thresholds[ix]\n",
    "print(threshold_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Threshold for Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_experimental, yhat)\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_prc = thresholds[ix]\n",
    "print(threshold_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics model calibrated - Choose the best calibration method before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best threshold to distinguishthe classes\n",
    "pred_rf = (probs_classes[:, 1] > threshold_roc).astype(int)\n",
    "# pred_rf = (probs_classes[:, 1] > threshold_prc).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_featmorgan_r2_2048_rf\"\n",
    "\n",
    "result_type = \"calibrated\"\n",
    "\n",
    "metrics_rf_calibrated = statistics\n",
    "metrics_rf_calibrated['model'] = model_type\n",
    "metrics_rf_calibrated['result_type'] = result_type\n",
    "metrics_rf_calibrated['calibration_threshold'] = threshold_roc\n",
    "metrics_rf_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe as excel file\n",
    "#metrics_rf_calibrated.to_excel(\"../results/model_binary_metrics_rf_featmorgan_r2_gmean_mpro_newdata_thres028.xlsx\", sheet_name= \"Sheet1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model - pkl file\n",
    "joblib.dump(opt_rf, \"../models/model_binary_hybrid_mordred_featmorgan_r2_2048_rf_gmean_1224857_bruno_imbalanced.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model building - Bayesian hyperparameter search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(geometric_mean_score)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt_svm = BayesSearchCV(\n",
    "    SVC(probability=True),\n",
    "    {\n",
    "        'C': (1e-6, 1e+6, 'log-uniform'),\n",
    "        'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
    "        'kernel': ['rbf'],  # categorical parameter | ['linear', 'poly', 'rbf'] to test all kernels\n",
    "    },\n",
    "    n_iter=50, # Number of parameter settings that are sampled\n",
    "    cv=cv,\n",
    "    scoring = scorer,\n",
    "    refit = True, # Refit the best estimator with the entire dataset.\n",
    "    random_state=42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "opt_svm.fit(x, y)\n",
    "\n",
    "print(\"Best parameters: %s\" % opt_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = []\n",
    "#indexes = []\n",
    "y_test_all = []\n",
    "\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    svm_clf = SVC(**opt_svm.best_params_, probability=True) # model with best parameters\n",
    "    X_train_folds = x[train_index] # descritors train split\n",
    "    y_train_folds = np.array(y)[train_index.astype(int)] # label train split\n",
    "    X_test_fold = x[test_index] # descritors test split\n",
    "    y_test_fold = np.array(y)[test_index.astype(int)] # label test split\n",
    "    \n",
    "    \n",
    "    svm_clf.fit(X_train_folds, y_train_folds) # train fold\n",
    "    y_pred = svm_clf.predict_proba(X_test_fold) # test fold\n",
    "    probs_classes.append(y_pred) # all predictions for test folds\n",
    "    y_test_all.append(y_test_fold) # all folds' labels \n",
    "#  indexes.append(test_index) # all tests indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of each fold\n",
    "fold_1_pred = (probs_classes[0][:, 1] > 0.5).astype(int)\n",
    "fold_2_pred = (probs_classes[1][:, 1] > 0.5).astype(int)\n",
    "fold_3_pred = (probs_classes[2][:, 1] > 0.5).astype(int)\n",
    "fold_4_pred = (probs_classes[3][:, 1] > 0.5).astype(int)\n",
    "fold_5_pred = (probs_classes[4][:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experimental values of each fold\n",
    "fold_1_exp = y_test_all[0]\n",
    "fold_2_exp = y_test_all[1]\n",
    "fold_3_exp = y_test_all[2]\n",
    "fold_4_exp = y_test_all[3]\n",
    "fold_5_exp = y_test_all[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacc1 = metrics.balanced_accuracy_score(fold_1_exp, fold_1_pred) # balanced accuracy fold 1\n",
    "bacc2 = metrics.balanced_accuracy_score(fold_2_exp, fold_2_pred) # balanced accuracy fold 2\n",
    "bacc3 = metrics.balanced_accuracy_score(fold_3_exp, fold_3_pred) # balanced accuracy fold 3\n",
    "bacc4 = metrics.balanced_accuracy_score(fold_4_exp, fold_4_pred) # balanced accuracy fold 4\n",
    "bacc5 = metrics.balanced_accuracy_score(fold_5_exp, fold_5_pred) # balanced accuracy fold 5\n",
    "print(\"Balanced accuracy (fold 1) = \", bacc1)\n",
    "print(\"Balanced accuracy (fold 2) = \", bacc2)\n",
    "print(\"Balanced accuracy (fold 3) = \", bacc3)\n",
    "print(\"Balanced accuracy (fold 4) = \", bacc4)\n",
    "print(\"Balanced accuracy (fold 5) = \", bacc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mean performance of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = np.concatenate(probs_classes)    \n",
    "y_experimental = np.concatenate(y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncalibrated model predictions\n",
    "pred_svm = (probs_classes[:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics - featmorgan-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncalibrated model predictions\n",
    "pred_svm = (probs_classes[:, 1] > 0.5).astype(int)\n",
    "len(y_experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_morgan_r2_2048_svm\"\n",
    "\n",
    "result_type = \"uncalibrated\"\n",
    "\n",
    "metrics_svm_uncalibrated = statistics\n",
    "metrics_svm_uncalibrated['model'] = model_type\n",
    "metrics_svm_uncalibrated['result_type'] = result_type\n",
    "metrics_svm_uncalibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model calibatrion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "probs = probs_classes[:, 1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_experimental, probs, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_model_hybrid_mordred_featmorgan_r2_2048_svm_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ROC-Curve and Gmean to select a threshold for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_experimental, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='SVM')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_curve_hybrid_mordred_featmorgan_r2_2048_svm_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_roc = thresholds[ix]\n",
    "print(threshold_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Threshold for Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_experimental, yhat)\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_prc = thresholds[ix]\n",
    "print(threshold_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics model calibrated - Choose the best calibration method before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best threshold to distinguishthe classes\n",
    "pred_svm = (probs_classes[:, 1] > threshold_roc).astype(int)\n",
    "# pred_svm = (probs_classes[:, 1] > threshold_prc).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_featmorgan_r2_2048_svm\"\n",
    "\n",
    "result_type = \"calibrated\"\n",
    "\n",
    "metrics_svm_calibrated = statistics\n",
    "metrics_svm_calibrated['model'] = model_type\n",
    "metrics_svm_calibrated['result_type'] = result_type\n",
    "metrics_svm_calibrated['calibration_threshold'] = threshold_roc\n",
    "metrics_svm_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe as excel file\n",
    "#metrics_svm.to_excel(\"../results/model_binary_metrics_svm_featmorgan_r2_gmean_mpro_newdata_thres039.xlsx\", sheet_name= \"Sheet1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model - pkl file\n",
    "joblib.dump(opt_svm, \"../models/model_binary_svm_hybrid_mordred_featmorgan_r2_2048_gmean_1224857_bruno_imbalanced.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model building - Bayesian hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(geometric_mean_score)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt_lgb = BayesSearchCV(lgb.LGBMClassifier(),\n",
    "                        {'learning_rate': (0.01, 1.0, 'log-uniform'), \n",
    "                         'num_leaves': (7, 4095),\n",
    "                         'n_estimators': (100, 800), \n",
    "                         'max_depth': (2, 63),\n",
    "                         'subsample': (0.4, 1), \n",
    "                         'scale_pos_weight': (1, 1000)}, \n",
    "                        n_iter = 50, # Number of parameter settings that are sampled\n",
    "                        cv = cv, \n",
    "                        scoring = scorer,\n",
    "                        refit = True, # Refit the best estimator with the entire dataset.\n",
    "                        verbose = 0,\n",
    "                        random_state = 42, \n",
    "                        n_jobs = 1)\n",
    "\n",
    "opt_lgb.fit(x, y)\n",
    "\n",
    "print(\"Best parameters: %s\" % opt_lgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = []\n",
    "#indexes = []\n",
    "y_test_all = []\n",
    "\n",
    "for train_index, test_index in cv.split(x, y):\n",
    "    lgb_clf = lgb.LGBMClassifier(**opt_lgb.best_params_,) # model with best parameters\n",
    "    X_train_folds = x[train_index] # descritors train split\n",
    "    y_train_folds = np.array(y)[train_index.astype(int)] # label train split\n",
    "    X_test_fold = x[test_index] # descritors test split\n",
    "    y_test_fold = np.array(y)[test_index.astype(int)] # label test split\n",
    "    \n",
    "    \n",
    "    svm_clf.fit(X_train_folds, y_train_folds) # train fold\n",
    "    y_pred = svm_clf.predict_proba(X_test_fold) # test fold\n",
    "    probs_classes.append(y_pred) # all predictions for test folds\n",
    "    y_test_all.append(y_test_fold) # all folds' labels \n",
    "#  indexes.append(test_index) # all tests indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of each fold\n",
    "fold_1_pred = (probs_classes[0][:, 1] > 0.5).astype(int)\n",
    "fold_2_pred = (probs_classes[1][:, 1] > 0.5).astype(int)\n",
    "fold_3_pred = (probs_classes[2][:, 1] > 0.5).astype(int)\n",
    "fold_4_pred = (probs_classes[3][:, 1] > 0.5).astype(int)\n",
    "fold_5_pred = (probs_classes[4][:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experimental values of each fold\n",
    "fold_1_exp = y_test_all[0]\n",
    "fold_2_exp = y_test_all[1]\n",
    "fold_3_exp = y_test_all[2]\n",
    "fold_4_exp = y_test_all[3]\n",
    "fold_5_exp = y_test_all[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacc1 = metrics.balanced_accuracy_score(fold_1_exp, fold_1_pred) # balanced accuracy fold 1\n",
    "bacc2 = metrics.balanced_accuracy_score(fold_2_exp, fold_2_pred) # balanced accuracy fold 2\n",
    "bacc3 = metrics.balanced_accuracy_score(fold_3_exp, fold_3_pred) # balanced accuracy fold 3\n",
    "bacc4 = metrics.balanced_accuracy_score(fold_4_exp, fold_4_pred) # balanced accuracy fold 4\n",
    "bacc5 = metrics.balanced_accuracy_score(fold_5_exp, fold_5_pred) # balanced accuracy fold 5\n",
    "print(\"Balanced accuracy (fold 1) = \", bacc1)\n",
    "print(\"Balanced accuracy (fold 2) = \", bacc2)\n",
    "print(\"Balanced accuracy (fold 3) = \", bacc3)\n",
    "print(\"Balanced accuracy (fold 4) = \", bacc4)\n",
    "print(\"Balanced accuracy (fold 5) = \", bacc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mean performance of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_classes = np.concatenate(probs_classes)    \n",
    "y_experimental = np.concatenate(y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncalibrated model predictions\n",
    "pred_lgb = (probs_classes[:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics - featmorgan-LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgb = (probs_classes[:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_featmorgan_r2_2048_lgb\"\n",
    "\n",
    "result_type = \"uncalibrated\"\n",
    "\n",
    "metrics_lgb_uncalibrated = statistics\n",
    "metrics_lgb_uncalibrated['model'] = model_type\n",
    "metrics_lgb_uncalibrated['result_type'] = result_type\n",
    "metrics_lgb_uncalibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model calibatrion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "probs = probs_classes[:, 1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_experimental, probs, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_model_hybrid_mordred_featmorgan_r2_2048_lgb_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ROC-Curve and Gmean to select a threshold for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_experimental, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='LightGBM')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "#plt.show()\n",
    "plt.savefig('../results/calibration_curve_hybrid_mordred_featmorgan_r2_2048_lgb_1224857_bruno_imbalanced.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_roc = thresholds[ix]\n",
    "print(threshold_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Threshold for Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = probs_classes[:, 1]\n",
    "# calculate precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_experimental, yhat)\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the threshold in a variable\n",
    "threshold_prc = thresholds[ix]\n",
    "print(threshold_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics model calibrated - Choose the best calibration method before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best threshold to distinguishthe classes\n",
    "pred_lgb = (probs_classes[:, 1] > threshold_roc).astype(int)\n",
    "# pred_lgb = (probs_classes[:, 1] > threshold_prc).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = calc_statistics(y_experimental, pred_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting calculated metrics into a pandas dataframe to save a xls\n",
    "model_type = \"hybrid_mordred_featmorgan_r2_2048_lgb\"\n",
    "\n",
    "result_type = \"calibrated\"\n",
    "\n",
    "metrics_lgb_calibrated = statistics\n",
    "metrics_lgb_calibrated['model'] = model_type\n",
    "metrics_lgb_calibrated['result_type'] = result_type\n",
    "metrics_lgb_calibrated['calibration_threshold'] = threshold_roc\n",
    "metrics_lgb_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe as excel file\n",
    "#metrics_lgb.to_excel(\"../results/model_binary_metrics_lgb_featmorgan_bee_mpro_newdata_gmean_NO_classweight_thres029.xlsx\", sheet_name= \"Sheet1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save an excell with all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [metrics_rf_uncalibrated, metrics_svm_uncalibrated, metrics_lgb_uncalibrated, \n",
    "          metrics_rf_calibrated, metrics_svm_calibrated, metrics_lgb_calibrated]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "\n",
    "column_names = [\"model\", \"Bal-acc\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Kappa\", \"MCC\", \"AUC\", \"F1-score\", \n",
    "                \"Classification error\", \"False positive rate\", \"False negative rate\", \"result_type\", \"calibration_threshold\"]\n",
    "result = result.reindex(columns=column_names)\n",
    "result = result.round(2)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe as excel file\n",
    "result.to_excel(\"../results/metrics_binary_hybrid_mordred_featmorgan_r2_2048_allmodels_1224857_bruno_imbalanced.xlsx\", sheet_name= \"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
